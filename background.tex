\documentclass{beamer}
\usepackage{latexsym}
\usepackage{graphicx}
\usetheme{Warsaw}

\title{Chapter 2 background}
\subtitle{Slides courtesy of \\Oliver Schulte and Greg Mori (Simon Fraser University)}

\begin{document}
\maketitle

\begin{frame}
  \frametitle{Coin Tossing}
  \begin{itemize}
  \item Given a coin, find out $P(heads)$
  \item I.e. the probability that if you flip it, it lands as `heads' \pause
  \item Flip it a few times: $H$ $H$ $T$
  \item $P(heads)=2/3$, no need for Comp 379
  \item Hmm... is this rigorous?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bernoulli distribution}
  \begin{itemize}
  \item Single binary random variable $x\in\{0,1\}$
  \item E.g. $x=1$ represents `heads' and $x=0$ represents `tails'
  \item Probability of $x=1$ denoted by the parameter $\mu$
  \item So, $p(x=1|\mu) = \mu$ and $p(x=0|\mu) = 1 - \mu$
  \item The probability distribution over $x$ can be written
  \end{itemize}
  \centering
  $Bern(x|\mu) = \mu^x(1-\mu)^{1-x}$
\end{frame}

\begin{frame}
  \frametitle{Coin Tossing - Model}
  \begin{itemize}
  \item Bernoulli distribution $P(heads) = \mu$, $P(tails) = 1 - \mu$
  \item Assume coin flips are independent and identically distributed (i.i.d.)
    \begin{itemize}
    \item I.e. All are separate samples from the Bernoulli distribution
    \end{itemize}
  \item Given data $\mathcal{D} = \{x_1,\ldots,x_N\}$, heads: $x_i=1$, tails: $x_i=0$, the likelihood of the data is: \[p(\mathcal{D}|\mu) = \prod_{n=1}^{N} p(x_n|\mu) = \prod_{n=1}^{N} \mu^{x_n} (1-\mu)^{1-x_n} \]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Maximum Likelihood Estimation}
  \begin{itemize}
    \item Given $\mathcal{D}$ with $h$ heads and $t$ tails
    \item What should $\mu$ be?
    \item Maximum Likelihood Estimation (MLE): choose $\mu$ which maximizes the likelihood of the data
      \[\mu_{ML} = \arg \max_{\mu} p(\mathcal{D}|\mu)
      \]
    \item Since $\ln(\cdot)$ is monotone increasing:
       \[\mu_{ML} = \arg \max_{\mu} \ln p(\mathcal{D}|\mu)
      \]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Maximum Likelihood Estimation}
  \begin{itemize}
  \item Likelihood:
    \[p(\mathcal{D}|\mu) = \prod_{n=1}^{N} \mu^{x_n} (1-\mu)^{1-x_n}\]
  \item Log-likelihood:
    \[\ln p(\mathcal{D}|\mu) = \sum_{n=1}^{N} x_n \ln \mu + (1-x_n) \ln (1-\mu)\]
  \item Take derivative, set to 0:
    \[\frac{d}{d \mu } \ln p(\mathcal{D}|\mu) = \sum_{n=1}^{N} x_n \frac{1}{\mu} - (1-x_n) \frac{1}{1-\mu}  = \frac{1}{\mu} h - \frac{1}{1-\mu} t\]
    \[\Rightarrow \mu = \frac{h}{t+h} \]
  \end{itemize}
\end{frame}

\end{document}
